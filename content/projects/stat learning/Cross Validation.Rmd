---
title: "Lab 02 - Cross Validation"
author: "Ellie Grace Moore"
date: "3/16/2022"
output: 
  html_document: 
    highlight: kate
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
library(tidymodels)
library(ISLR)
```

The k-fold cross validation approach is implemented by first randomly splitting the data into k equal-sized parts. Then, leaves out one of these parts, then fits a model to the k-1 parts and then uses the part we left out to calculate the test error of the model. This process is repeated until we left out every piece of data, and then we take the k values for test error, then average them in order to calculate our final estimate for the test error for the model.

Some advantages of k-fold cross validation compared to the validation set (when the data is randomly divided into two equal parts) appraoch is that we are able to use more data to fit our model to, and we are able to calculate numerous values for the test error and then take the average for an ideally better estimate. The validation set approach on the other hand only uses roughly half the data to fit the model to and it can be highly variable depending on which specific subset we choose to fit the model to. An advantage to the validation set approach is that we are working with (assumably) much more data when calculating the test error compared to the k-fold cv. Now, for LOOCV, we use just about the entire data set to fit the model (except for the point we left out) which results in a much less biased model than k-fold cv and validation set and has no randomness involved. However a key disadvantage to LOOCV is that it can potentially take much longer to compute since we are doing a k-fold cv for when k=n. Another disadvantage is that it has a higher variance, since we are averaging n values for the test error. 

```{r}
set.seed(100)
Auto_split  <- initial_split(Auto, prop = 0.5)
Auto_train  <- training(Auto_split)
Auto_test   <- testing(Auto_split)
```

```{r}
lm_spec <-
  linear_reg() %>%
  set_engine("lm")
vs_fit_1      <- fit(lm_spec, 
                   mpg ~ horsepower, 
                   data = Auto_train)
mpg_pred_1 <- vs_fit_1 %>% 
  predict(new_data = Auto) %>% 
  bind_cols(Auto)
mpg_pred_1
```

```{r}
mpg_pred_1 %>%
rmse(truth = mpg, estimate = .pred)
```

```{r}
vs_fit_2      <- fit(lm_spec, 
                   mpg ~ poly(horsepower, 2), 
                   data = Auto_train)
mpg_pred_2 <- vs_fit_2 %>% 
  predict(new_data = Auto) %>% 
  bind_cols(Auto)
mpg_pred_2

mpg_pred_2 %>%
rmse(truth = mpg, estimate = .pred)
```

```{r}
vs_fit_3      <- fit(lm_spec, 
                   mpg ~ poly(horsepower, 3), 
                   data = Auto_train)
mpg_pred_3 <- vs_fit_3 %>% 
  predict(new_data = Auto) %>% 
  bind_cols(Auto)
mpg_pred_3

mpg_pred_3 %>%
rmse(truth = mpg, estimate = .pred)
```

```{r}
set.seed(75)
Auto_cv <- vfold_cv(Auto, v = 5)

results_1 <- fit_resamples(lm_spec,
                         mpg ~ horsepower, 
                         resamples = Auto_cv)

results_1 %>%
  collect_metrics()

```


```{r}
results_2 <- fit_resamples(lm_spec,
                         mpg ~ poly(horsepower, 2), 
                         resamples = Auto_cv)

results_2 %>%
  collect_metrics()
```


```{r}
auto_prep <- Auto %>%
  recipe(mpg ~ horsepower) %>%
  step_poly(horsepower, degree = tune())

auto_prep
```

```{r}
auto_tune <- tune_grid(lm_spec,
          auto_prep,
          resamples = Auto_cv)
auto_tune %>%
  collect_metrics()
```

```{r}
auto_metrics <- auto_tune %>%
  collect_metrics()

auto_rmse <- 
  filter(auto_metrics, .metric == "rmse")

ggplot(auto_rmse, aes(x = degree, y = mean)) + 
  geom_line(color = "chartreuse3") +
  geom_pointrange(ymin = 0, ymax = 3, color = "chartreuse3") +
  labs(x = "Degree",
       y = "Cross Validation Error",
       title = "Error Comparisons for Different Degrees of 5-Fold Cross Validation") +
  theme(plot.title = element_text(face = "bold", hjust = .5))
```

